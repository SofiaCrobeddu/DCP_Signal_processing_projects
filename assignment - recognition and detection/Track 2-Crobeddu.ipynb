{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Track 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source of the video used to check this algorithm and to set the rectangle's measures, to the following link: https://github.com/intel-iot-devkit/sample-videos?tab=readme-ov-file\n",
    "\n",
    "File name used: `head-pose-face-detection-male.mp4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, read well!\n"
     ]
    }
   ],
   "source": [
    "#Loading the video\n",
    "videoReader = cv2.VideoCapture('C:\\\\Users\\\\sofyc\\\\OneDrive\\\\Desktop\\\\UPEC\\\\CV II\\\\assignment - recognition and detection\\\\head-pose-face-detection-male.mp4')\n",
    "\n",
    "#Check\n",
    "if not videoReader.isOpened():\n",
    "    print(f\"Error: the video can't be loaded.\")\n",
    "else:\n",
    "    print(\"Okay, read well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 1 with CAMShift\n",
    "In this algorithm we have a continuous detection, i.e. dynamic detection.\n",
    "- Use of **CAMShift (Continuously Adaptive Mean Shift)**:\n",
    "CAMShift is an algorithm used to track objects in a video, and it is based on an algorithm called Mean shift.\n",
    "  - **Mean Shift**: it searches in a iterative way the local maximum of a probability distribution, given the pixels of a certain specific area. It identifies the centroid, i.e. the mass center of the pixels with highest probability.\n",
    "\n",
    "CAMShift is a kind of extention of Mean Shift, since it is more flexible adapting the window dimension at each change in the detection. It takes a color distribution based on the histogram HSV color space, and returns the tracked changes withing time. In our case, since we are detecting a face, the color of the skin and the hair is constant within time and it can be tracked easily.\n",
    "\n",
    "- **Region of Interest (ROI)**:\n",
    "it is a part of the image (face) that is isolated to calculate the color histogram HSV used by the CAMShift detection.\n",
    "\n",
    "- **HSV (Hue Saturation Value)**:\n",
    "it is a color space different from RGB (Red, Green, Blue).\n",
    "  - **Hue**: it is the color component between 0 and 180 in OpenCV.\n",
    "  - **Saturation**: it is the intensity of color and the higher the saturation, the higher the intensity.\n",
    "  - **Value**: it is the brightness of the color and the higher the value, the higher the brightness.\n",
    "It is useful since it lets to track the color besides the brightness. Hence, it is easier to detect a face. We convert the ROI region in HSV in order to create an histogram to represent the distribution of the colors in the intensity of the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sofyc\\OneDrive\\Desktop\\environment for NN and ML\\myenv\\Lib\\site-packages\\cv2\\data\\\n"
     ]
    }
   ],
   "source": [
    "#Looking at the path for the files of detection\n",
    "print(cv2.data.haarcascades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load three pre-trained classifiers from OpenCV\n",
    "#Face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "#Eye detection\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "#Nose detection\n",
    "nose_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')\n",
    "\n",
    "#Loop to read and process each frame of the video\n",
    "ret, frame = videoReader.read()\n",
    "if not ret:\n",
    "    print(\"Error: it was not possible to read the video.\")\n",
    "    videoReader.release()\n",
    "    exit()\n",
    "if frame is None:  #Empty frame\n",
    "    print(\"Empty frame, going to the next one.\")\n",
    "    videoReader.release()\n",
    "    exit()\n",
    "\n",
    "#Converting the frame to grayscale\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#Detection of face as a rectangle\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(120, 120))\n",
    "\n",
    "#Checking to see if faces were detected\n",
    "if len(faces) > 0:\n",
    "    (x, y, w, h) = faces[0]  #Taking the first face detected\n",
    "\n",
    "    #Extracting the face Region of Interest (ROI)\n",
    "    roi = frame[y:y + h, x:x + w]\n",
    "    hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)  #Converting the ROI in HSV color space.\n",
    "\n",
    "    #Creating a histogram of the ROI\n",
    "    roi_hist = cv2.calcHist([hsv_roi], [0], None, [256], [0, 180])\n",
    "    cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    #Set up the tracking window for CAMShift\n",
    "    track_window = (x, y, w, h)\n",
    "\n",
    "    #Loop through the video frames\n",
    "    while True:\n",
    "        #Reading next frame\n",
    "        ret, frame = videoReader.read()\n",
    "        if not ret:\n",
    "            break  #Exit if there are no frames anymore\n",
    "\n",
    "        #Calculating the back projection\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        dst = cv2.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
    "\n",
    "        #Criteria for CAMShift\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)  #10 iterations or accuracy of 1\n",
    "\n",
    "        #Applying CAMShift to get the new location of the face\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, criteria)\n",
    "        x, y, w, h = track_window  #Updating the coordinates of the tracked face\n",
    "\n",
    "        #Updating the ROI for eyes and nose detection in the new tracked location\n",
    "        face_roi_gray = cv2.cvtColor(frame[y:y + h, x:x + w], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        #Detect eyes and nose within the updated face ROI\n",
    "        eyes = eye_cascade.detectMultiScale(face_roi_gray, scaleFactor=1.1, minNeighbors=10, minSize=(20, 20))\n",
    "        nose = nose_cascade.detectMultiScale(face_roi_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        #Drawing rectangles for the eyes (only the first two)\n",
    "        for (ex, ey, ew, eh) in eyes[:2]:  #Limit to two eyes\n",
    "            cv2.rectangle(frame, (x + ex, y + ey), (x + ex + ew, y + ey + eh), (0, 255, 0), 2)  #Green Rectangle\n",
    "\n",
    "        #Drawing a rectangle for the nose if detected\n",
    "        if len(nose) > 0:\n",
    "            (nx, ny, nw, nh) = nose[0]  #Taking the first nose detected\n",
    "            cv2.rectangle(frame, (x + nx, y + ny - 5), (x + nx + nw, y + ny + nh - 5), (0, 0, 255), 2)  #Red Rectangle\n",
    "\n",
    "        #Show the frames\n",
    "        cv2.imshow(\"Face Tracking\", frame)\n",
    "\n",
    "        if cv2.waitKey(30) & 0xFF == ord('q'):  #Click 'q' to exit loop.\n",
    "            break\n",
    "\n",
    "#Release resources\n",
    "videoReader.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative 2\n",
    "Static detection. We use the haar cascades of OpenCV in evry single frame totally from 0, to detect face, eyes and nose. There is no continuity between a frame and the successive one. Basically, it doesn't maintain the information of the detection of a frame for the successive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I load three pre-trained classifiers from OpenCV\n",
    "#Face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "#Eye detection\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "#Nose detection\n",
    "nose_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')\n",
    "\n",
    "#Loop to read and process each frame of the video\n",
    "while True:\n",
    "    ret, frame = videoReader.read()\n",
    "    if not ret:\n",
    "        print(\"Error: it was not possible to read the video.\")\n",
    "        break\n",
    "\n",
    "    if frame is None: #Empty frame\n",
    "        print(\"Empty frame, going to the next one.\")\n",
    "        continue\n",
    "\n",
    "    #Converting the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Detection of face as a rectangle\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(120, 120))\n",
    "\n",
    "    #Checking to see if faces were detected\n",
    "    if len(faces) > 0:\n",
    "        (face_x, face_y, face_w, face_h) = faces[0]  #Taking the first face detected\n",
    "\n",
    "        #Detecting eyes within the face region\n",
    "        face_roi_gray = gray[face_y:face_y + face_h, face_x:face_x + face_w]\n",
    "        eyes = eye_cascade.detectMultiScale(face_roi_gray, scaleFactor=1.1, minNeighbors=10, minSize=(20, 20))\n",
    "\n",
    "        #Draw a rectangle for each detected eye in green\n",
    "        for (ex, ey, ew, eh) in eyes[:2]:  #We limit at 2 eyes\n",
    "            #Reducing the rectangle dimensions\n",
    "            cv2.rectangle(frame, (face_x + ex + 4, face_y + ey + 4), \n",
    "                          (face_x + ex + ew - 4, face_y + ey + eh - 4), (0, 255, 0), 2)\n",
    "\n",
    "        #Detecting the nose within the face region in red\n",
    "        nose = nose_cascade.detectMultiScale(face_roi_gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "        if len(nose) > 0:\n",
    "            (nx, ny, nw, nh) = nose[0]  #Taking the first nose detected\n",
    "            vertical_offset = 4  #Value to shift the rectangle up or down in vertical sense\n",
    "            cv2.rectangle(frame, (face_x + nx + 6, face_y + ny - vertical_offset), \n",
    "                  (face_x + nx + nw - 6, face_y + ny + nh - 5 - vertical_offset), (0, 0, 255), 2)\n",
    "    \n",
    "    #Show the frames\n",
    "    cv2.imshow(\"Face Detection with Eyes and Nose\", frame)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'): #Click 'q' to exit loop.\n",
    "        break\n",
    "\n",
    "#Release resources\n",
    "videoReader.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
